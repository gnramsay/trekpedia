{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df20ede",
   "metadata": {},
   "source": [
    "# Trekpedia\n",
    "\n",
    "\n",
    "Writing a web-scraper to pull all `Star Trek(tm)` series data from Wikipedia.\n",
    "\n",
    "## Stage 2 - get episode data\n",
    "Create separate json files containing episode data for each Series.\n",
    "For now we will keep all seasons in one file but may break this into individual ones depending on how much data we finally grab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1159ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common setup...\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# don't truncate Pandas.DataFrame cell contents when displaying.\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0313e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from stage 1 ...\n",
    "df = pd.read_json('../output/star_trek_series_info.json', orient='index')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset that only contains the seasons we want to work on.\n",
    "# df2 = pd.DataFrame(df.iloc[6]).transpose()\n",
    "df2 = df\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58211808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to clean up strings - remove unicode and any brackets\n",
    "def clean_string(str, underscores=False, brackets=False, lowercase=False):\n",
    "    print(str)\n",
    "    if underscores:\n",
    "        str = str.replace(\" \", \"_\").replace(\".\", \"_\").replace(\"__\", \"_\")\n",
    "    if brackets:\n",
    "        str = \"\".join(re.split(\"\\(|\\)|\\[|\\]\", str)[::2])\n",
    "    if lowercase:\n",
    "        str = str.lower()\n",
    "    return ' '.join(str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b807c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : The Original Series\n",
      "  -> Using URL : https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes\n",
      "  -> Storing episodes to '../output/star_trek_series_1_the_original_series_episodes.json'\n",
      "  -> Processing season: 1 of 3\n",
      "September 8, 1966 (1966-09-08)\n",
      "[<td colspan=\"2\">29</td>, <td colspan=\"1\" style=\"padding:0.2em 0.4em\">September 8, 1966<span style=\"display:none\"> (<span class=\"bday dtstart published updated\">1966-09-08</span>)</span></td>, <td style=\"padding:0 8px\">April 13, 1967<span style=\"display:none\"> (<span class=\"dtend\">1967-04-13</span>)</span></td>]\n",
      "April 13, 1967 (1967-04-13)\n",
      "  -> Done.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-box/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb#ch0000005vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(cells)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-box/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb#ch0000005vscode-remote?line=53'>54</a>\u001b[0m season_data[\u001b[39m'\u001b[39m\u001b[39mseason_start\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clean_string(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(cells[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39msplit()), brackets\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdev-box/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb#ch0000005vscode-remote?line=54'>55</a>\u001b[0m season_data[\u001b[39m'\u001b[39m\u001b[39mseason_end\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clean_string(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(cells[\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39msplit()), brackets\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-box/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb#ch0000005vscode-remote?line=55'>56</a>\u001b[0m season_data[\u001b[39m'\u001b[39m\u001b[39mepisodes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-box/home/seapagan/work/own-gnramsay/trekpedia/notebooks/trekpedia-stage-2.ipynb#ch0000005vscode-remote?line=57'>58</a>\u001b[0m \u001b[39m# now get the actual episodes for this season...\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "FILE_TEMPLATE = '../output/star_trek_series_{}_{}_episodes.json'\n",
    "\n",
    "for i, row in enumerate(df2.itertuples(),1):  \n",
    "    \n",
    "    if row.name in ['Prodigy']:\n",
    "        continue\n",
    "\n",
    "    print(f'Processing : {row.name}')\n",
    "    filename = FILE_TEMPLATE.format(i,row.name.replace(\" \", \"_\").lower())\n",
    "    print(f\"  -> Using URL : {row.episodes_url}\")\n",
    "    print(f\"  -> Storing episodes to '{filename}'\")\n",
    "    \n",
    "    season_final = dict()\n",
    "    season_all = dict()\n",
    "    \n",
    "    # get and parse the webpage...\n",
    "    result = requests.get(row.episodes_url)\n",
    "    bs = BeautifulSoup(result.text, 'lxml')\n",
    "\n",
    "    # wrap all this in a Try:Except block, there are a few series which need special handling...\n",
    "    try:\n",
    "        # find the episode summary table, will be the first table with the below classes in the document\n",
    "        summary_table = bs.find('table', attrs={'class': 'wikitable plainrowheaders'})\n",
    "\n",
    "        if summary_table:\n",
    "            summary_rows = summary_table.find('tbody').find_all('tr')[2:]\n",
    "        else:\n",
    "            print(\"   x No Summary Table found, currently skipping this Series ...\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        for season in summary_rows:\n",
    "            season_data = dict()\n",
    "            link = season.find('th')\n",
    "            cells = season.find_all('td')         \n",
    "            \n",
    "            try:\n",
    "                season_number = int(link.text)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            \n",
    "            # exit the loop if we have processed the actual number of seasons. Usually this is not needed, \n",
    "            # however it is for the new series that are still in progress. \n",
    "            if season_number > row.season_count:\n",
    "                break\n",
    "            \n",
    "            print(f'  -> Processing season: {season_number} of {row.season_count}')\n",
    "            season_id = link.a['href'][1:]\n",
    "            season_data['total'] = clean_string(cells[1].text, brackets=True)\n",
    "            # get start/end data and remove unicode chars. \n",
    "            # Still need to remove the date in backets at the end of each\n",
    "            print(cells)\n",
    "            season_data['season_start'] = clean_string(\" \".join(cells[2].text.split()), brackets=True)\n",
    "            season_data['season_end'] = clean_string(\" \".join(cells[3].text.split()), brackets=True)\n",
    "            season_data['episodes'] = list()\n",
    "\n",
    "            # now get the actual episodes for this season...\n",
    "            section = bs.find('span', id=season_id)\n",
    "            table = section.findNext('table').find('tbody').find_all('tr')\n",
    "            \n",
    "            # split the headers out into a list, as they change between series and even seasons!\n",
    "            # at this time we also remove any unicode stuff \n",
    "            h = table[0].find_all('th')        \n",
    "            headers = [clean_string(x.text, underscores=True, brackets=True, lowercase=True) for x in h]\n",
    "            # remove the overall count as this is a TH not a TD and will skew the indexing later...\n",
    "            headers.remove('no_overall')\n",
    "            \n",
    "            # 'episodes' will consist of one row for each episode, except ds9 and voy who also put summary\n",
    "            # after each one and confuse things!\n",
    "            episodes = table[1:]\n",
    "        \n",
    "            episode_list = list()\n",
    "            # loop over each episode, getting the relevant data. We may grab more info in the future.\n",
    "            for episode in episodes:\n",
    "                episode_data = dict()\n",
    "                # protect the next operation - if the th is not found (ie tas, ds9, voy) just skip over this \n",
    "                # one as it is a summary...\n",
    "                try:\n",
    "                    episode_data['num_overall'] = clean_string(episode.find('th').text, brackets=True)\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "                cells = episode.find_all('td')\n",
    "                episode_data['num_in_season'] = cells[headers.index('no_inseason')].text\n",
    "                \n",
    "                # need to do some tweaking, sometimes the first episode is in 2 parts\n",
    "                # need to detect this and split them. Alternative is to have a hard-coded list, as it\n",
    "                # happens very rarely.\n",
    "                \n",
    "                # get the required data using the header indexes, otherwise will mess up on ds9-s4 and later\n",
    "                # since they add new columns to the table.\n",
    "                episode_data['title'] = clean_string(cells[headers.index('title')].text.replace('\"',''), brackets=True)\n",
    "                try:\n",
    "                    # put these in try/except as some don't have episode links\n",
    "                    \n",
    "                    link_url = cells[headers.index('title')].a['href']\n",
    "                    if \"cite_note\" in link_url:\n",
    "                        raise TypeError() \n",
    "                    episode_data['link'] = f\"https://en.wikipedia.org{link_url}\"\n",
    "                except TypeError:\n",
    "                    # set the link url to an empty string...\n",
    "                    episode_data['link'] = ''\n",
    "                \n",
    "                episode_data['director'] = clean_string(cells[headers.index('directed_by')].text, brackets=True)\n",
    "\n",
    "                # air date needs a regex as is listed differently in later series...\n",
    "                airdate_idx = [i for i, item in enumerate(headers) if re.search('^original.*date$', item)][0]\n",
    "                episode_data['air_date'] = clean_string(cells[airdate_idx].text, brackets=True)\n",
    "                \n",
    "                episode_list.append(episode_data)\n",
    "                \n",
    "            # consolidate into a format suitable for writing to JSON\n",
    "            season_data['episodes'] = episode_list\n",
    "            season_all[season_number] = season_data\n",
    "            season_final['seasons'] = season_all\n",
    "    except AttributeError as e:\n",
    "        print(f\"  => ERROR, need to investigate! ({e}) at line number: {e.__traceback__.tb_lineno}\")\n",
    "    finally:\n",
    "       # write to json file...\n",
    "        with open (filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(season_final, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"  -> Done.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92a1e0",
   "metadata": {},
   "source": [
    "## Current Bugs\n",
    "1. Some 2-part episodes have bad season and overall number due to table layout.\n",
    "2. [`ALL FIXED`] At least DS9 from season 4 and Voyager, Enterprise add a\n",
    "   'stardate' column which messes up the column count and therefore the\n",
    "   'Original Air Date' field. Voyager also adds 'featured character' to this\n",
    "   confusion. Later the air date field is renamed too.\n",
    "3. [`FIXED`] Discovery errors out after first season\n",
    "4. [`ALL FIXED`] From Short Treks to Lower Decks error out on line 76, more\n",
    "   formatting changes.\n",
    "5. Prodigy errors out at the start, this is because it has no Season Summary\n",
    "   table. We may want to change the way we get the data, using the\n",
    "   'wikiepisodetable' class directly. For now I'm skipping this as it prob needs\n",
    "   a rewrite of the base logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6bba4",
   "metadata": {},
   "source": [
    "# Different Method test.\n",
    "Leaving this in as markdown for future reference, but the read_html function of Pandas is not really working well for these tables.\n",
    "\n",
    "```python\n",
    "FILE_TEMPLATE = 'output/star_trek_series_{}_{}_episodes.json'\n",
    "\n",
    "for row in df.itertuples(index=True):\n",
    "   \n",
    "    print(f'Processing : {row.name}')\n",
    "    filename = FILE_TEMPLATE.format(row.Index,row.name.replace(\" \", \"_\").lower())\n",
    "    print(f\"  -> Using URL : {row.episodes_url}\")\n",
    "    print(f\"  -> Storing episodes to '{filename}'\")\n",
    "    \n",
    "    season_final = dict()\n",
    "    season_all = dict()\n",
    "    \n",
    "    # get and parse the webpage...\n",
    "    result = requests.get(row.episodes_url)\n",
    "    bs = BeautifulSoup(result.text, 'lxml')\n",
    "    \n",
    "    # wrap all this in a Try:Except block, there are a few series which need special handling...\n",
    "    try:\n",
    "        # find the episode summary table, will be the first table with the below classes in the document\n",
    "        summary_table = bs.find('table', attrs={'class': 'wikitable plainrowheaders'})\n",
    "        \n",
    "        summary_rows = summary_table.find('tbody').find_all('tr')[2:]\n",
    "        \n",
    "        for season in summary_rows:\n",
    "            season_data = dict()\n",
    "            \n",
    "            link = season.find('th')\n",
    "            cells = season.find_all('td')         \n",
    "            \n",
    "            season_number = link.text\n",
    "            season_id = link.a['href'][1:]\n",
    "            season_data['total'] = cells[1].text\n",
    "            # get start/end data and remove unicode chars. \n",
    "            # Still need to remove the date in backets at the end of each\n",
    "            season_data['start'] = \" \".join(cells[2].text.split())\n",
    "            season_data['end'] = \" \".join(cells[3].text.split())\n",
    "            season_data['episodes'] = list()\n",
    "            \n",
    "            # now get the actual episodes for this season...\n",
    "            section = bs.find('span', id=season_id)\n",
    "            table = section.findNext('table')\n",
    "            \n",
    "            table_data = pd.read_html(str(table), parse_dates=True)\n",
    "            print(table_data)\n",
    "            print(\"[>----------------<]\")\n",
    "            \n",
    "    except AttributeError as e:\n",
    "        print(f\"  => Error, need to investigate! ({e}) at line number: {e.__traceback__.tb_lineno}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f975eca457c413ecfffa3b26ee5bfb96b52c7a248ca235d083f835b868b010c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
